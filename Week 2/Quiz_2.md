# Week 2 Quiz 

* Question 1
- [x] a<sup>[3]{8}(7)
#
* Question 2
- [x] One iteration of mini-batch gradient descent (computing on a single mini-batch) is faster than one iteration of batch gradient descent.
#
* Question 3
- [x] If the mini-batch size is 1, you lose the benefits of vectorization across examples in the mini-batch.
- [x] If the mini-batch size is m, you end up with batch gradient descent, which has to process the whole training set before making progress.
#
* Question 4
- [x] If you’re using mini-batch gradient descent, this looks acceptable. But if you’re using batch gradient descent, something is wrong.
